# T-CRIS: Temporal Cancer Recurrence Intelligence System

**An AI-Powered Platform for Bladder Cancer Recurrence Prediction and Treatment Optimization**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

---

## ğŸ¯ Project Overview

T-CRIS is a comprehensive, production-ready AI platform that combines classical survival analysis with modern deep learning for bladder cancer recurrence prediction. It features personalized treatment recommendations, interpretable AI explanations, and interactive visualizations.

### Novel Contributions

1. **Hybrid Statistical-DL Framework**: Seamlessly combines Cox PH, Anderson-Gill, Random Survival Forests, LSTM, and Transformer models
2. **Multi-Format Data Fusion**: Automatic unification of WLW, Anderson-Gill, and standard survival data formats
3. **Counterfactual Treatment Analysis**: Personalized "what-if" scenarios for treatment selection
4. **Competing Risks Neural Network**: Multi-task deep learning for recurrence + death events
5. **Attention-Based Temporal Mining**: Discovers which past recurrences predict future risk
6. **Interactive Dashboard + REST API**: Production-ready deployment with FastAPI and Streamlit

---

## ğŸ“ Project Structure

```
project-bcrs/
â”œâ”€â”€ data/                           # Data directory
â”‚   â”œâ”€â”€ raw/                        # Original CSV files (bladder.csv, bladder1.csv, bladder2.csv)
â”‚   â”œâ”€â”€ processed/                  # Cleaned, unified data
â”‚   â”œâ”€â”€ features/                   # Engineered features
â”‚   â””â”€â”€ validation/                 # Train/test splits
â”‚
â”œâ”€â”€ src/tcris/                      # Main package
â”‚   â”œâ”€â”€ config/                     # Configuration management
â”‚   â”‚   â”œâ”€â”€ settings.py             # Pydantic settings (single source of truth)
â”‚   â”‚   â””â”€â”€ logging.yaml            # Logging configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                       # Data layer
â”‚   â”‚   â”œâ”€â”€ loaders.py              # Unified CSV loader (DRY principle)
â”‚   â”‚   â”œâ”€â”€ validators.py           # Data validation (Great Expectations)
â”‚   â”‚   â”œâ”€â”€ fusion.py               # Multi-format data fusion
â”‚   â”‚   â”œâ”€â”€ preprocessors.py        # Data preprocessing
â”‚   â”‚   â””â”€â”€ augmentation.py         # Data augmentation
â”‚   â”‚
â”‚   â”œâ”€â”€ features/                   # Feature engineering
â”‚   â”‚   â”œâ”€â”€ extractors.py           # Feature extraction
â”‚   â”‚   â”œâ”€â”€ transformers.py         # sklearn-compatible transformers
â”‚   â”‚   â””â”€â”€ temporal.py             # Time-dependent features
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                     # Model layer
â”‚   â”‚   â”œâ”€â”€ base.py                 # Abstract base classes (DRY)
â”‚   â”‚   â”œâ”€â”€ statistical/            # Classical survival models
â”‚   â”‚   â”‚   â”œâ”€â”€ cox.py              # Cox Proportional Hazards
â”‚   â”‚   â”‚   â”œâ”€â”€ anderson_gill.py    # Anderson-Gill recurrent events
â”‚   â”‚   â”‚   â””â”€â”€ competing_risks.py  # Fine-Gray competing risks
â”‚   â”‚   â”œâ”€â”€ machine_learning/       # ML models
â”‚   â”‚   â”‚   â”œâ”€â”€ random_survival_forest.py
â”‚   â”‚   â”‚   â””â”€â”€ gradient_boosting.py
â”‚   â”‚   â”œâ”€â”€ deep_learning/          # Deep learning models
â”‚   â”‚   â”‚   â”œâ”€â”€ lstm_temporal.py    # LSTM for recurrence sequences
â”‚   â”‚   â”‚   â”œâ”€â”€ transformer.py      # Transformer with attention
â”‚   â”‚   â”‚   â”œâ”€â”€ competing_risks_nn.py  # Multi-task competing risks
â”‚   â”‚   â”‚   â””â”€â”€ bayesian_survival.py   # Bayesian uncertainty quantification
â”‚   â”‚   â””â”€â”€ ensemble/               # Ensemble methods
â”‚   â”‚       â”œâ”€â”€ stacking.py
â”‚   â”‚       â””â”€â”€ meta_model.py
â”‚   â”‚
â”‚   â”œâ”€â”€ prediction/                 # Application layer
â”‚   â”‚   â”œâ”€â”€ predictor.py            # Main prediction engine
â”‚   â”‚   â”œâ”€â”€ counterfactual.py       # Treatment comparison
â”‚   â”‚   â””â”€â”€ risk_trajectory.py      # Dynamic risk evolution
â”‚   â”‚
â”‚   â”œâ”€â”€ interpretation/             # Interpretability
â”‚   â”‚   â”œâ”€â”€ shap_explainer.py       # SHAP explanations
â”‚   â”‚   â”œâ”€â”€ lime_explainer.py       # LIME local explanations
â”‚   â”‚   â””â”€â”€ attention_viz.py        # Attention visualization
â”‚   â”‚
â”‚   â”œâ”€â”€ similarity/                 # Patient similarity
â”‚   â”‚   â”œâ”€â”€ distance_metrics.py
â”‚   â”‚   â””â”€â”€ clustering.py
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/                 # Model evaluation
â”‚   â”‚   â”œâ”€â”€ metrics.py              # C-index, Brier score, etc.
â”‚   â”‚   â”œâ”€â”€ calibration.py          # Calibration plots
â”‚   â”‚   â””â”€â”€ validators.py           # Cross-validation
â”‚   â”‚
â”‚   â”œâ”€â”€ visualization/              # Visualization layer
â”‚   â”‚   â”œâ”€â”€ survival_plots.py       # Kaplan-Meier, hazard plots
â”‚   â”‚   â”œâ”€â”€ risk_plots.py           # Risk trajectories
â”‚   â”‚   â”œâ”€â”€ interpretability_plots.py  # SHAP, attention plots
â”‚   â”‚   â””â”€â”€ dashboard_components.py    # Reusable UI components
â”‚   â”‚
â”‚   â”œâ”€â”€ reports/                    # Report generation
â”‚   â”‚   â”œâ”€â”€ statistical_report.py
â”‚   â”‚   â”œâ”€â”€ patient_report.py
â”‚   â”‚   â””â”€â”€ templates/              # LaTeX/Jinja templates
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                        # FastAPI REST API
â”‚   â”‚   â”œâ”€â”€ main.py                 # FastAPI app
â”‚   â”‚   â”œâ”€â”€ routes/                 # API routes
â”‚   â”‚   â”‚   â”œâ”€â”€ prediction.py
â”‚   â”‚   â”‚   â”œâ”€â”€ analysis.py
â”‚   â”‚   â”‚   â””â”€â”€ reports.py
â”‚   â”‚   â””â”€â”€ schemas.py              # Pydantic models (DRY)
â”‚   â”‚
â”‚   â””â”€â”€ utils/                      # Utilities
â”‚       â”œâ”€â”€ decorators.py           # Common decorators (DRY)
â”‚       â”œâ”€â”€ exceptions.py           # Custom exceptions
â”‚       â””â”€â”€ helpers.py              # Helper functions
â”‚
â”œâ”€â”€ dashboard/                      # Streamlit dashboard
â”‚   â”œâ”€â”€ app.py                      # Main Streamlit app
â”‚   â”œâ”€â”€ pages/                      # Multi-page app
â”‚   â”‚   â”œâ”€â”€ 01_overview.py
â”‚   â”‚   â”œâ”€â”€ 02_survival_analysis.py
â”‚   â”‚   â”œâ”€â”€ 03_predictions.py
â”‚   â”‚   â”œâ”€â”€ 04_counterfactual.py
â”‚   â”‚   â””â”€â”€ 05_interpretability.py
â”‚   â””â”€â”€ components/                 # Reusable UI components (DRY)
â”‚
â”œâ”€â”€ notebooks/                      # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_statistical_analysis.ipynb
â”‚   â”œâ”€â”€ 03_model_development.ipynb
â”‚   â”œâ”€â”€ 04_model_evaluation.ipynb
â”‚   â””â”€â”€ 05_presentation_figures.ipynb
â”‚
â”œâ”€â”€ tests/                          # Tests
â”‚   â”œâ”€â”€ unit/                       # Unit tests
â”‚   â”œâ”€â”€ integration/                # Integration tests
â”‚   â””â”€â”€ fixtures/                   # Test data (DRY)
â”‚
â”œâ”€â”€ scripts/                        # CLI scripts
â”‚   â”œâ”€â”€ train_models.py             # Training pipeline
â”‚   â”œâ”€â”€ generate_report.py          # Report generator
â”‚   â””â”€â”€ validate_data.py            # Data validation
â”‚
â”œâ”€â”€ models/                         # Saved model artifacts
â”‚   â”œâ”€â”€ statistical/
â”‚   â”œâ”€â”€ deep_learning/
â”‚   â””â”€â”€ ensemble/
â”‚
â”œâ”€â”€ outputs/                        # Generated outputs
â”‚   â”œâ”€â”€ reports/                    # PDF reports
â”‚   â”œâ”€â”€ figures/                    # Visualizations
â”‚   â””â”€â”€ predictions/                # Prediction results
â”‚
â”œâ”€â”€ docs/                           # Documentation
â”‚   â”œâ”€â”€ api/                        # API documentation
â”‚   â”œâ”€â”€ user_guide/                 # User guide
â”‚   â””â”€â”€ technical/                  # Technical documentation
â”‚
â”œâ”€â”€ pyproject.toml                  # Poetry dependencies
â”œâ”€â”€ Makefile                        # Common commands (KISS)
â”œâ”€â”€ .env.example                    # Environment variables template
â”œâ”€â”€ README.md                       # Dataset information
â”œâ”€â”€ DATA_INFO.md                    # Detailed data documentation
â””â”€â”€ PROJECT_README.md               # This file
```

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10 or higher
- Poetry (for dependency management)

### Installation

1. **Clone the repository**:
   ```bash
   git clone <repository-url>
   cd project-bcrs
   ```

2. **Install dependencies**:
   ```bash
   make install
   # or manually:
   poetry install
   ```

3. **Set up environment**:
   ```bash
   make setup
   # This copies .env.example to .env
   # Edit .env with your configuration
   ```

4. **Validate data**:
   ```bash
   make validate-data
   ```

### Usage

#### 1. Train Models
```bash
make train
```

#### 2. Run API Server
```bash
make api
# Access Swagger docs at http://localhost:8000/docs
```

#### 3. Launch Dashboard
```bash
make dashboard
# Opens at http://localhost:8501
```

#### 4. Run Jupyter Notebooks
```bash
make notebook
```

---

## ğŸ¨ Key Features

### 1. Data Processing
- **Multi-Format Support**: Handles WLW, Anderson-Gill, and standard formats
- **Automatic Format Detection**: Intelligently detects data format
- **Data Validation**: Comprehensive quality checks with Great Expectations
- **Feature Engineering**: 20+ engineered features (temporal, tumor progression, interactions)

### 2. Modeling

#### Statistical Models
- Cox Proportional Hazards
- Anderson-Gill Frailty Model
- Wei-Lin-Weissfeld Marginal Model
- Fine-Gray Competing Risks Model
- Aalen Additive Model

#### Machine Learning Models
- Random Survival Forest
- Gradient Boosting Survival Analysis

#### Deep Learning Models
- LSTM Temporal Recurrence Model
- Transformer with Attention Mechanism
- Competing Risks Neural Network
- Bayesian Survival Network

#### Ensemble
- Stacking ensemble combining all models
- Optimized meta-model

### 3. Prediction & Analysis
- **Individual Risk Prediction**: Patient-specific recurrence risk scores
- **Survival Curves**: Time-dependent survival probabilities
- **Counterfactual Analysis**: "What-if" treatment scenarios
- **Dynamic Risk Trajectories**: Risk evolution over time
- **Uncertainty Quantification**: Confidence intervals and credible regions

### 4. Interpretability
- **SHAP Values**: Feature importance for each prediction
- **LIME Explanations**: Local model behavior
- **Attention Visualization**: Which past events matter most
- **Feature Importance**: Global model understanding

### 5. Interactive Dashboard

#### Pages:
1. **Overview**: Dataset summary, statistics
2. **Survival Analysis**: Kaplan-Meier curves, log-rank tests
3. **Predictions**: Individual patient risk assessment
4. **Counterfactual**: Treatment comparison
5. **Interpretability**: Model explanations, feature importance

### 6. REST API

#### Endpoints:
- `POST /api/v1/predict` - Get recurrence prediction
- `POST /api/v1/predict/batch` - Batch predictions
- `POST /api/v1/counterfactual` - Treatment comparison
- `GET /api/v1/survival_curve` - Survival curves
- `POST /api/v1/similar_patients` - Find similar patients
- `GET /api/v1/models` - List available models
- `POST /api/v1/reports/generate` - Generate reports

---

## ğŸ“Š Datasets

The project uses three bladder cancer recurrence datasets:

1. **bladder.csv**: WLW format, 85 patients, up to 4 recurrences
2. **bladder1.csv**: Extended WLW, 118 patients, up to 9 recurrences
3. **bladder2.csv**: Anderson-Gill format, 85 patients

See [README.md](README.md) and [DATA_INFO.md](DATA_INFO.md) for detailed information.

---

## ğŸ§ª Testing

```bash
# Run all tests with coverage
make test

# Run quick tests (no coverage)
make test-quick

# Run specific test file
poetry run pytest tests/unit/test_data_loaders.py
```

Target coverage: >80%

---

## ğŸ“ Code Quality

### Formatting
```bash
# Format code
make format

# Check formatting
make format-check
```

### Linting
```bash
make lint
```

### All Quality Checks
```bash
make all
```

---

## ğŸ“ Design Principles

### KISS (Keep It Simple, Stupid)
- Simple, consistent APIs (`.fit()`, `.predict()`)
- Minimal dependencies
- Clear, self-documenting code
- Flat module structure

### DRY (Don't Repeat Yourself)
- Base classes define common interfaces once
- Shared utilities in `utils/` module
- Single data loader handles all formats
- Reusable dashboard components
- Pydantic Settings as single source of truth

---

## ğŸ“ˆ Model Performance

### Target Metrics
- **C-index** (discrimination): >0.70
- **Integrated Brier Score**: <0.20
- **Calibration slope**: ~1.0
- **Time-dependent AUC at 1 year**: >0.75
- **API latency**: <500ms per prediction

---

## ğŸ¤ Presentation Highlights

1. **Live Demo**: Interactive dashboard with real-time predictions
2. **Attention Visualization**: Heatmaps showing temporal patterns learned by transformer
3. **Counterfactual Analysis**: Side-by-side treatment comparison for personalized medicine
4. **Model Performance**: Ensemble outperforms individual models
5. **Interpretability**: SHAP waterfall plots explaining predictions to clinicians

---

## ğŸ“š Documentation

- **User Guide**: `docs/user_guide/`
- **API Documentation**: `docs/api/` (auto-generated from docstrings)
- **Technical Documentation**: `docs/technical/`
- **API Swagger UI**: http://localhost:8000/docs (when API is running)

To build docs:
```bash
make docs
```

To serve docs locally:
```bash
make docs-serve
```

---

## ğŸ› ï¸ Development

### Project Phases

#### Phase 1: Foundation (Weeks 1-2) âœ…
- [x] Project structure
- [x] Configuration management
- [x] Data loading and validation
- [ ] Feature engineering
- [ ] Statistical models
- [ ] Basic dashboard

#### Phase 2: Advanced Analytics (Weeks 3-4)
- [ ] ML models (RSF, GBM)
- [ ] DL models (LSTM, Transformer)
- [ ] REST API
- [ ] Model evaluation framework

#### Phase 3: Novel Features (Weeks 5-6)
- [ ] Counterfactual analysis
- [ ] Interpretability (SHAP, LIME, attention)
- [ ] Patient similarity engine
- [ ] Advanced visualizations

#### Phase 4: Polish & Presentation (Week 7)
- [ ] Report generation
- [ ] Dashboard enhancement
- [ ] Comprehensive documentation
- [ ] Jupyter notebooks

#### Phase 5: Testing & Validation (Week 8)
- [ ] Unit tests (>80% coverage)
- [ ] Integration tests
- [ ] External validation
- [ ] Presentation preparation

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Run tests and quality checks (`make all`)
5. Commit your changes (`git commit -m 'Add amazing feature'`)
6. Push to the branch (`git push origin feature/amazing-feature`)
7. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## ğŸ™ Acknowledgments

- Dataset: Bladder cancer recurrence data from medical research
- Inspiration: Combining classical biostatistics with modern AI for precision medicine
- Tools: Python ecosystem (pandas, scikit-learn, PyTorch, Streamlit, FastAPI)

---

## ğŸ“§ Contact

For questions, feedback, or collaboration:
- Open an issue on GitHub
- Email: [your.email@example.com]

---

## ğŸ¯ Project Status

**Current Status**: Phase 1 - Foundation âœ…
**Next Milestone**: Feature Engineering & Statistical Models

**Progress**:
- [x] Project structure and configuration
- [x] Data loading infrastructure
- [x] Multi-format data fusion
- [ ] Feature engineering pipeline
- [ ] Statistical models implementation
- [ ] Dashboard MVP

---

**Built with â¤ï¸ for advancing precision medicine through AI**
